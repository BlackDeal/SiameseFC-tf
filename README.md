# SiamFC-tf
This repository is the tensorflow implementation of both training and evaluation of SiamFC described in the paper [*Fully-Convolutional Siamese nets for object tracking*](https://www.robots.ox.ac.uk/~luca/siamese-fc.html).   
The code is revised on the base of the evaluation-only version code from the repository of the auther of this paper(https://github.com/torrvision/siamfc-tf).

## Author of this repository
**Zhenghao Zhao**

## Authors of the original paper
* [**Luca Bertinetto**](https://www.robots.ox.ac.uk/~luca)
* [**Jack Valmadre**](http://jack.valmadre.net)


## 2 Training Step

### 2.1 Prepare training data
One training sample consists of an examplar image: **z**, a search image : **x**, and their correspnding ground truth information: **z_pos_x, z_pos_y, z_target_w, z_target_h and x_pos_x, x_pos_y, x_target_w, x_target_h**. Note the coordinates of the target have been converted to the center of bbox from the lefttop corner through function *src.region_to_bbox.py*. <br>
<br>
We pick the neibored two images in a vedio as z and x, and get a shuffled training data set from all 78 vedios in ImageNet Large Scale Visual Recognition Competition (ILSVRC) data set. And for conveniece of later steps, we resize all images to a uniform size [*design.resize_width, design.resize_height*]. The training data set is saved in a tfrecord file.

### 2.2 Pad & Crop the image
Before entering the conv network, we crop **z** and **x** to certain sizes, and pad with the mean RGB value of each image if the crop region exceeds the orignal image size.<br>
<br>
According to the paper, the croped **z** should contains the area of the marginal bbox on the original image, where the margin *p = (w + h) / 4*, and this marginal context will then be resized to a constant size: [*design.exemplar_sz, design.exemplar_sz*], which is [127, 127] in our training process. <br>
<br>
Cropping step of **x** is the same, but only all the sizes are larger by a facter: *desing.search_sz / desing_examplar_sz* and the center of cropping is the center of target position in **z** instead of **x**. In addition, cause the evaluation process needs to inference three different scales of **x** to implement the update on target scale, here in training process, we also crop three **x** samples, but with the same size. (The last sample actually has a little bit larger size due to the way we crop the image, but this doesn't matter.)

### 2.3 Siamese conv net
When entering the conv net, **x** tensor has a shape of [*batch_size × 3, desing.search_sz, desing.search_sz, channel*], and **z** tensor has a shape of [*batch_size, design.exemplar_sz, design.exemplar_sz, channel*]. <br>
<br>
**x** and **z** will go to the same conv net, for which we adopt a Alexnet architecture. The parameters of the net are defined in *design.filter_w, ...* and *src.siamese._conv_stride, ...*.<br>
<br>
To match the shape of triple scaled **x** feature map, we then stack the **z** feature map for three times.

### 2.4 Cross Correlation
We calcualte a score map by implementing a cross correlation between the extracted feature map of **z** and **x**.<br><br>
First the feature map of **z** is transposed and reshaped to [*w, h, batch_size × feature_channel × 3, 1*], and the feature map of **x** is reshaped to [*1, w, h, batch_size × feature_channel × 3*]. <br>
<br>
Then we calculate the depthwise_conv2d of **x** feature with respect to **z** feature as the filter. Output tensor of this step has a shape of [*1, w, h, batch_size × feature_channel × 3*].<br>
<br>
This output tensor will further be splited and concatebated to a final shape of [*batch_size × 3, w, h, feature_channel*]. Then the final score map is generated by reduce_sum through all the feature_channels.

### 2.5 Metrics
The score map is resized to the same size of the cropped **x** image by bilinear interpolation, from which we pick the top-3 scores and calculate their average coordinate as the predicted target position. Metrics such as distance to ground truth can then be calculated.<br>
<br>
The logistic loss of the predicted score map is defined as: $\frac{1}{D}\sum_{u \in D}\log{(1 + e^{-y(u)v(u)})}$, where *D* represent all the pixels on the score map. And the value of the ground thruth label *v(u)* is *+1* if it is within the ground thruth bbox, and *-1* if not.<br>
<br>
We adopt the adamoptimizer with an initial learning rate of *1e-6* to minimize the loss.

# User Guide for the code
## Preparation
1) Clone the repository
`git clone https://github.com/zzh142857/SiameseFC-tf.git`   
2)The code is prepared for a environment with Python==3.6 and Tensorflow-gpu==1.6 with the required CUDA and CudNN library.   
3)Other packages can be installed by:   
`sudo pip install -r requirements.txt`   
4)Download training data   
cd to the directory where this README.md file located, then:
`mkdir data`   
Download [video sequences](https://drive.google.com/file/d/0B7Awq_aAemXQSnhBVW5LNmNvUU0/view) in `data` and unzip the archive.

## Training model
1)Generate tf


## Running the tracker
1) Set `video` from `parameters.evaluation` to `"all"` or to a specific sequence (e.g. `"vot2016_ball1"`)
1) See if you are happy with the default parameters in `parameters/hyperparameters.json`
1) Optionally enable visualization in `parameters/run.json`
1) Call the main script (within an active virtualenv session)
`python run_tracker_evaluation.py`


## References
If you find our work useful, please consider citing the original authors

↓ [Original method] ↓
```
@inproceedings{bertinetto2016fully,
  title={Fully-Convolutional Siamese Networks for Object Tracking},
  author={Bertinetto, Luca and Valmadre, Jack and Henriques, Jo{\~a}o F and Vedaldi, Andrea and Torr, Philip H S},
  booktitle={ECCV 2016 Workshops},
  pages={850--865},
  year={2016}
}
```
↓ [Improved method and evaluation] ↓
```
@article{valmadre2017end,
  title={End-to-end representation learning for Correlation Filter based tracking},
  author={Valmadre, Jack and Bertinetto, Luca and Henriques, Jo{\~a}o F and Vedaldi, Andrea and Torr, Philip HS},
  journal={arXiv preprint arXiv:1704.06036},
  year={2017}
}
```

## License
This code can be freely used for personal, academic, or educational purposes.
Please contact us for commercial use.

